{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3573a47-3689-4668-b62f-5c8451b2b4e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all dependencies listed in the 'requirements.txt' file with their specified versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24af50c-20b8-409d-ad78-30a933fdd669",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3db0b0-e559-4ad6-91fd-e7414b7d75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is Present\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d045a-3003-4f93-b7d2-a25a97774a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prevent Exponential Memory Growth\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19e88e-c7b9-45c1-ae1e-f2109329c71b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Build Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019e4c6-2af3-4160-99ea-5c8cb009f1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downloading and Extracting the Dataset\n",
    "\n",
    "# import gdown\n",
    "# url = 'https://drive.google.com/file/d/1czmd2kDblp7t6wambFSFf5YgzU5k85kV/view?usp=sharing'\n",
    "# output = 'dataset.zip'\n",
    "# gdown.download(url=url, output=output, fuzzy=True)\n",
    "# gdown.extractall('dataset.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec735e0b-ec98-4eb0-8f49-c35527d6670a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]\n",
    "print(vocab, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be04e972-d7a5-4a72-82d8-a6bdde1f3ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\") \n",
    "num_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True) \n",
    "\n",
    "print(f'The vocabulary is: {char_to_num.get_vocabulary()} ' \n",
    "      f'(size ={char_to_num.vocabulary_size()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ff78b-b48f-4e14-bb62-8cd0ebf9501a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing fn\n",
    "# char_to_num(['s','j','a','i', 'n']) #[19, 10,  1,  9, 14]\n",
    "# num_to_char([19, 10,  1,  9, 14]) #['s','j','a','i', 'n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548cc59-6dfc-4acc-abc3-3e65212db02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the video\n",
    "def load_video(path:str) -> List[float]: \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))): \n",
    "        ret, frame = cap.read()\n",
    "        frame = tf.image.rgb_to_grayscale(frame)\n",
    "        frames.append(frame[190:236,80:220,:]) \n",
    "    cap.release()\n",
    "    \n",
    "    mean = tf.math.reduce_mean(frames)\n",
    "    std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n",
    "    return tf.cast((frames - mean), tf.float32) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491bab5-6a3c-4f79-879a-8f9fbe73ae2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the subtitles for video\n",
    "def load_subtitles(path:str) -> List[str]: \n",
    "    with (open(path, 'r') as f): \n",
    "        lines = f.readlines() \n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        if line[2] != 'sil': \n",
    "            tokens = [*tokens,' ',line[2]]\n",
    "    return char_to_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01ca9f-77fb-4643-a2aa-47dd82c5d66b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading the video and subtitles simultaneously\n",
    "def load_data(path: str): \n",
    "    path = bytes.decode(path.numpy()) \n",
    "\n",
    "    # Path Splitter for MacOS/Linux\n",
    "    file_name = path.split('/')[-1].split('.')[0]\n",
    "\n",
    "    # for Windows\n",
    "    # file_name = path.split('\\\\')[-1].split('.')[0]\n",
    "    \n",
    "    video_path = os.path.join('dataset','videos',f'{file_name}.mpg') # replace with your location\n",
    "    subtitles_path = os.path.join('dataset','subtitles',f'{file_name}.align') # replace with your location\n",
    "    frames = load_video(video_path) \n",
    "    subtitles = load_subtitles(subtitles_path)\n",
    "    \n",
    "    return frames, subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb7cc58-31ae-4904-a805-1177a82717d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_path = 'dataset/videos/bbal6n.mpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa964f-0c84-490d-897a-d00e3966e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.convert_to_tensor(test_path).numpy().decode('utf-8').split('/')[-1].split('.')[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ac6d0f5814c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(tf.convert_to_tensor(test_path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb602c71-8560-4f9e-b26b-08202febb937",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames, subtitles = load_data(tf.convert_to_tensor(test_path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3184a1-6b02-4b4f-84a8-a0a65f951ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the function\n",
    "plt.imshow(frames[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ad370-b287-4b46-85a2-7c45b0bd9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strings.reduce_join([bytes.decode(x) for x in num_to_char(subtitles.numpy()).numpy()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871031a-b0ba-4c76-a852-f6329b0f2606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mappable_function(path:str) -> List[str]:\n",
    "    result = tf.py_function(load_data, [path], (tf.float32, tf.int64))\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a7eb4-0c3e-4eab-9291-5611cb68ce08",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Create Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686355d-45aa-4c85-ad9c-053e6a9b4d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f066fea2-91b1-42ed-a67d-00566a1a53ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.list_files('dataset/videos/*.mpg') # replace with your location\n",
    "data = data.shuffle(500, reshuffle_each_iteration=False) \n",
    "data = data.map(mappable_function)\n",
    "data = data.padded_batch(2, padded_shapes=([75,None,None,None],[40]))\n",
    "data = data.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Added for split \n",
    "train = data.take(450)\n",
    "test = data.skip(450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1365bd-7742-41d1-95d4-247021751c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5281bde8-fdc8-4da1-bd55-5a7929a9e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, subtitles = data.as_numpy_iterator().next() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbebe683-6afd-47fd-bba4-c83b4b13bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frames), len(subtitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2d676-93a9-434c-b3c7-bdcc2577b2e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6cd46-7079-46c0-b45b-832f339f6cb0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "val = sample.next(); val[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf5eb4f-a0da-4a9a-bf24-af13e9cc2fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imageio.mimsave('animation.gif', val[0][1], fps=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a87a2-d5e0-4ec9-b174-73ebf41bf03a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0:videos, 0: 1st video out of the batch,  0: return the frame in the video \n",
    "plt.imshow(val[0][0][34]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84593332-133c-4205-b7a6-8e235d5e2b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.strings.reduce_join([num_to_char(word) for word in val[1][0]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47733c-83bc-465c-b118-b198b492ad37",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Design the Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9a497-191b-4842-afbd-26f5e13c43ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten \n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler \n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f753ed2-70b9-4236-8c1c-08ca065dc8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape of the input\n",
    "data.as_numpy_iterator().next()[0][0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9171056-a352-491a-9ed9-92b28ced268e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add a 3D convolutional layer with 128 filters, kernel size of 3, and input shape of (75, 46, 140, 1)\n",
    "model.add(Conv3D(128, 3, input_shape=(75,46,140,1), padding='same'))  \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1,2,2)))  # Apply 3D max pooling with pool size of (1, 2, 2)\n",
    "\n",
    "# Add another 3D convolutional layer with 256 filters and kernel size of 3\n",
    "model.add(Conv3D(256, 3, padding='same')) \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1,2,2)))  # Apply 3D max pooling with pool size of (1, 2, 2)\n",
    "\n",
    "# Add another 3D convolutional layer with 75 filters and kernel size of 3\n",
    "model.add(Conv3D(75, 3, padding='same'))  \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1,2,2)))  # Apply 3D max pooling with pool size of (1, 2, 2)\n",
    "\n",
    "# Apply TimeDistributed layer to flatten the input\n",
    "model.add(TimeDistributed(Flatten()))  \n",
    "\n",
    "# Define an Orthogonal initializer\n",
    "initializer = keras.initializers.Orthogonal()\n",
    "\n",
    "# Add a bidirectional LSTM layer with 128 units, using Orthogonal initializer and returning sequences\n",
    "model.add(Bidirectional(LSTM(128, kernel_initializer=initializer, return_sequences=True)))  \n",
    "model.add(Dropout(.5))  # Apply dropout with a rate of 0.5\n",
    "\n",
    "# Add another bidirectional LSTM layer with 128 units, using Orthogonal initializer and returning sequences\n",
    "model.add(Bidirectional(LSTM(128, kernel_initializer=initializer, return_sequences=True)))  \n",
    "model.add(Dropout(.5))  # Apply dropout with a rate of 0.5\n",
    "\n",
    "# Add a dense layer with units equal to the vocabulary size plus 1, using He normal initializer and softmax activation function\n",
    "model.add(Dense(char_to_num.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78851825-2bcd-42a9-b7f2-28bb5a6bf43a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2eae0-c359-41a4-97a0-75c44dccb7d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yhat = model.predict(val[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc7319-0d69-4f7e-a6d4-ce72deb81c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strings.reduce_join([num_to_char(x) for x in tf.argmax(yhat[0],axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed47531-8317-4255-9a12-b757642258e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.strings.reduce_join([num_to_char(tf.argmax(x)) for x in yhat[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c37b9b9-5298-4038-9c33-5031d1b457f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b316a4-5322-4782-8e36-4b3c1a696d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec02176-5c26-46c3-aff7-8352e6563c7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Setup Training Options and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab015fd0-7fb4-4d5d-9fa2-30a05dbd515a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 30:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c564d5c9-db54-4e88-b311-9aeab7fb3e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CTCLoss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\") \n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\") \n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\") \n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\") \n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\") \n",
    "\n",
    "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26dc3fc-a19c-4378-bd8c-e2b597a1d15c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProduceExample(tf.keras.callbacks.Callback): \n",
    "    def __init__(self, dataset) -> None: \n",
    "        self.dataset = dataset.as_numpy_iterator()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None) -> None:\n",
    "        data = self.dataset.next()\n",
    "        yhat = self.model.predict(data[0]) \n",
    "        decoded = tf.keras.backend.ctc_decode(yhat, [75,75], greedy=False)[0][0].numpy()\n",
    "        for x in range(len(yhat)):           \n",
    "            print('Original:', tf.strings.reduce_join(num_to_char(data[1][x])).numpy().decode('utf-8'))\n",
    "            print('Prediction:', tf.strings.reduce_join(num_to_char(decoded[x])).numpy().decode('utf-8'))\n",
    "            print('~'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be90d8-2482-46f9-b513-d5f4f8001c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.legacy.Adam(learning_rate=0.0001), loss=CTCLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab49367-3f1e-4464-ae76-dbd07549d97e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(os.path.join('model','checkpoint'), monitor='loss', save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085a632-d464-46ef-8777-959cad4adb2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schedule_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eca991-90ab-4592-8a79-b50e9ca015b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_callback = ProduceExample(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffba483-aa61-4bbe-a15f-a73e1ddf097c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(train, validation_data=test, epochs=100, callbacks=[checkpoint_callback, schedule_callback, example_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ee94b-89f7-4733-8a0c-a86f86ff590a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Make a Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a43876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and Extracting my Pre-Trained Model\n",
    "\n",
    "# import gdown\n",
    "# url = 'https://drive.google.com/file/d/1eg4FaZgTPF6vlFJHhreAdxY48cgKezER/view?usp=sharing'\n",
    "# output = 'models.zip'\n",
    "# gdown.download(url=url, output=output, fuzzy=True)\n",
    "# gdown.extractall('models.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f664d-3c87-4e96-946e-930dad0e1c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights('models/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d689f-b7bb-443c-9b88-e40c1d800828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38546dc2-bee9-4837-864b-8a884df40ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = test_data.next()\n",
    "yhat = model.predict(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea462999-f87e-4a7e-a057-5be7b6d8f7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('~'*50, 'REAL TEXT', '~'*50)\n",
    "[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in sample[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d68ac46-c90b-4eab-a709-f19aee569ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75,75], greedy=True)[0][0].numpy()\n",
    "print('~'*50, 'PREDICTIONS', '~'*50)\n",
    "[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64622f98-e99b-4fed-a2cc-f0da82eb5431",
   "metadata": {},
   "source": [
    "# Test on a Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0c4d0-2031-4331-b91d-d87b1ae6f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = load_data(tf.convert_to_tensor('/dataset/videos/lgbf9s.mpg')) # replace with your location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca60e4-47a9-4683-8a75-48f4684f723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('~'*50, 'REAL TEXT', '~'*50)\n",
    "[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in [sample[1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5037c-1e32-435c-b0cc-01e1fb3b863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(tf.expand_dims(sample[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d12ecc-b634-499e-a4bc-db9f010835fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75], greedy=True)[0][0].numpy()\n",
    "print('~'*50, 'PREDICTIONS', '~'*50)\n",
    "[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea3724",
   "metadata": {},
   "source": [
    "# Accuracy Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a220c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import editdistance\n",
    "\n",
    "def cer(preds, labels):\n",
    "    total_chars = 0\n",
    "    total_errors = 0\n",
    "    for pred, label in zip(preds, labels):\n",
    "        total_chars += len(label)\n",
    "        total_errors += editdistance.eval(pred, label)\n",
    "    \n",
    "    return total_errors / total_chars\n",
    "\n",
    "def wer(preds, labels):\n",
    "    total_words = 0\n",
    "    total_errors = 0\n",
    "    for pred, label in zip(preds, labels):\n",
    "        pred_words = pred.split()\n",
    "        label_words = label.split()\n",
    "        \n",
    "        total_words += len(label_words)\n",
    "        total_errors += editdistance.eval(pred_words, label_words)\n",
    "    \n",
    "    return total_errors / total_words\n",
    "\n",
    "def decode_predictions(predictions):\n",
    "    decoded = tf.keras.backend.ctc_decode(predictions, input_length=[predictions.shape[1]] * predictions.shape[0], greedy=True)[0][0].numpy()\n",
    "    decoded_str = [tf.strings.reduce_join([num_to_char(word) for word in sentence]).numpy().decode('utf-8') for sentence in decoded]\n",
    "    return decoded_str\n",
    "\n",
    "# Load your test data\n",
    "test_data = test.as_numpy_iterator()\n",
    "sample = test_data.next()\n",
    "\n",
    "# Model predictions\n",
    "yhat = model.predict(sample[0])\n",
    "\n",
    "# Decode predictions and true labels\n",
    "decoded_preds = decode_predictions(yhat)\n",
    "true_labels = [tf.strings.reduce_join([num_to_char(word) for word in sentence]).numpy().decode('utf-8') for sentence in sample[1]]\n",
    "\n",
    "# Compute CER and WER\n",
    "cer_value = cer(decoded_preds, true_labels)\n",
    "wer_value = wer(decoded_preds, true_labels)\n",
    "\n",
    "print(f'Character Error Rate (CER): {cer_value:.2f}')\n",
    "print(f'Word Error Rate (WER): {wer_value:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
